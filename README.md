# 6650_final_project


# ğŸƒ Rule-based Baseline & Shallow Q-Learning

This module implements and evaluates two essential baseline agents for the Texas Holdâ€™em environment (`pettingzoo.classic.texas_holdem_v4`):

1. **Rule-based Policy (hand-crafted expert system)**
2. **Shallow RL Agent (Tabular Q-learning)**

These baselines provide reference performance before introducing Deep RL (PPO).

---

# ğŸ“¦ Environment Setup

```python
from pettingzoo.classic import texas_holdem_v4
```

Each agent receives:

* A 52-bit card visibility vector
* A legal action mask
* Actions: `0=Call`, `1=Raise`, `2=Fold`, `3=Check`

---

# ğŸ¯ 1. Rule-based Policy (Baseline)

## State Representation

We extract:

* **Street**: preflop, flop, turn, river â†’ 4 levels
* **Hand strength**: weak, medium, strong â†’ 3 levels

## Policy Logic

| Street          | Strength | Action Priority      |
| --------------- | -------- | -------------------- |
| Preflop         | Strong   | Raise â†’ Call â†’ Check |
| Preflop         | Medium   | Call â†’ Check         |
| Preflop         | Weak     | Check â†’ Fold         |
| Flop/Turn/River | Strong   | Raise â†’ Call â†’ Check |
| Flop/Turn/River | Medium   | Call â†’ Check         |
| Flop/Turn/River | Weak     | Check â†’ Fold         |

---

# ğŸ“Š Rule-based Action Distribution

![Rule-based Action Distribution](rule_based_action_dist.png)

---

# ğŸ† Rule-based Performance vs Random

| Agent      | Win Rate | Tie Rate | Loss Rate | Mean Reward |
| ---------- | -------- | -------- | --------- | ----------- |
| Rule-based | 0.600    | 0.012    | 0.388     | 0.568       |

---

# ğŸ¤– 2. Shallow RL Agent â€” Tabular Q-learning

## State Encoding

12 discrete states formed by:

* 4 street levels
* 3 strength levels

## Q-learning Settings

| Parameter       | Value      |
| --------------- | ---------- |
| Episodes        | 5000       |
| Learning rate Î± | 0.1        |
| Discount Î³      | 0.9        |
| Îµ-greedy        | 0.2 â†’ 0.05 |
| Opponent        | Random     |

---

# ğŸ“ˆ Learning Curve

![Learning Curve](q_learning_curve.png)

---

# ğŸ”¥ Q-table Heatmap

![Q-table Heatmap](q_table_heatmap.png)

---

# ğŸ… Shallow Q-learning Performance vs Random

| Agent              | Win Rate | Tie Rate | Loss Rate | Mean Reward |
| ------------------ | -------- | -------- | --------- | ----------- |
| Shallow Q-learning | 0.954    | 0.004    | 0.042     | 1.990       |

---

# âš”ï¸ Rule-based vs Shallow Q-learning (Head-to-Head)

| Agent              | Mean Reward | Win Rate |
| ------------------ | ----------- | -------- |
| Rule-based         | 0.568       | 0.600    |
| Shallow Q-learning | 1.990       | 0.954    |

---

# ğŸ“š Summary of Completed Work

âœ” Rule-based policy
âœ” Rule-based action distribution plot
âœ” Rule-based performance table
âœ” Shallow Q-learning implementation
âœ” Q-learning learning curve
âœ” Q-table heatmap
âœ” Shallow Q-learning performance table
âœ” Rule-based vs shallow Q-learning comparison table

This baseline module is fully completed.

---

# ğŸ“ File Structure

| File                      | Description       |
| ------------------------- | ----------------- |
| `rule_based.py`           | Rule-based policy |
| `shallow_q.py`            | Q-learning agent  |
| `baseline_analysis.ipynb` | Full evaluation   |
| `plots/`                  | Figures           |
| `README.md`               | Documentation     |

---

# â–¶ï¸ How to Run

Open:

```
Baseline Policies/RuleBased_Shallow.ipynb
```

Running all cells will generate:

* Action distribution
* Learning curve
* Q-table heatmap
* Performance comparison tables


