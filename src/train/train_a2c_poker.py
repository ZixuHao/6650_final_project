# ============================================================
# train_a2c_poker.py
# A2C training on unified Poker-v0 environment (PettingZoo Texas Hold'em)
# Assumes poker_env.py has already registered "Poker-v0"
# ============================================================

import os
import ray

from ray.tune.registry import register_env
from ray.rllib.algorithms.a2c import A2CConfig

# âš ï¸ æ ¹æ®ä½ çš„è·¯å¾„ä¿®æ”¹è¿™ä¸€è¡Œï¼š
# å¦‚æœ poker_env.py åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå°±ç”¨ä¸‹é¢è¿™ä¸€è¡Œ
from src.env.poker_env import env_creator

# å¦‚æœä½ æ˜¯æ”¾åœ¨ src/env/poker_env.pyï¼Œå¯ä»¥æ”¹æˆï¼š
# from src.env.poker_env import env_creator


# ------------------------------------------------------------
# 1. æ³¨å†Œç¯å¢ƒï¼ˆå¦‚æœåœ¨ poker_env.py é‡Œå·²ç» register è¿‡ï¼Œè¿™é‡Œå†å†™ä¸€éä¹Ÿæ²¡å…³ç³»ï¼‰
# ------------------------------------------------------------
def _env_creator(config=None):
    return env_creator(config or {})

register_env("Poker-v0", _env_creator)


# ------------------------------------------------------------
# 2. åˆå§‹åŒ– Ray
# ------------------------------------------------------------
ray.init(ignore_reinit_error=True)


# ------------------------------------------------------------
# 3. é…ç½® A2Cï¼šå…±äº«ä¸€ä¸ª policy ç»™ä¸¤ä¸ªç©å®¶
# ------------------------------------------------------------
config = (
    A2CConfig()
    .environment(env="Poker-v0")
    .framework("torch")
    .rollouts(
        num_rollout_workers=2,      # ä½ ç”µè„‘å¾ˆå¡çš„è¯å¯ä»¥æ”¹æˆ 1
        rollout_fragment_length=50
    )
    .training(
        gamma=0.99,
        lr=7e-4,
        use_critic=True,
        use_gae=True,
        grad_clip=0.5,
    )
    .resources(
        num_gpus=0                  # å¦‚æœæœ‰ GPU å¯ä»¥æ”¹æˆ 1
    )
)

# Multi-agent è®¾ç½®ï¼šæ‰€æœ‰ agent éƒ½ç”¨åŒä¸€ä¸ª policyï¼ˆshared_policyï¼‰
config = config.multi_agent(
    policies={
        "shared_policy": (
            None,   # policy class (None -> default)
            None,   # obs space (None -> infer from env)
            None,   # act space (None -> infer from env)
            {},     # policy config
        )
    },
    policy_mapping_fn=lambda agent_id, *args, **kwargs: "shared_policy",
    policies_to_train=["shared_policy"],
)


# ------------------------------------------------------------
# 4. æ„å»º A2C ç®—æ³•
# ------------------------------------------------------------
algo = config.build()

# åˆ›å»ºä¿å­˜ checkpoint çš„ç›®å½•
checkpoint_dir = "checkpoints_a2c"
os.makedirs(checkpoint_dir, exist_ok=True)


# ------------------------------------------------------------
# 5. è®­ç»ƒå¾ªç¯
# ------------------------------------------------------------
num_iters = 80        # å¯ä»¥å…ˆè·‘ 20â€“50 çœ‹æ•ˆæœï¼Œå†åŠ 
save_every = 10       # æ¯éš”å¤šå°‘ iter å­˜ä¸€æ¬¡æ¨¡å‹

for i in range(num_iters):
    result = algo.train()
    mean_reward = result.get("episode_reward_mean", None)
    episodes = result.get("episodes_this_iter", None)

    print(f"Iter {i+1:03d} | "
          f"mean_reward={mean_reward:.3f} | "
          f"episodes={episodes}")

    # æ¯éš”ä¸€æ®µæ—¶é—´å­˜ä¸ª checkpoint
    if (i + 1) % save_every == 0:
        ckpt_path = algo.save(checkpoint_dir)
        print(f"  ğŸ’¾ Saved checkpoint at: {ckpt_path}")

print("\nâœ… A2C training finished.")

ray.shutdown()
