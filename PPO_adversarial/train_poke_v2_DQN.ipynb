{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2285bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from pettingzoo.classic import texas_holdem_v4\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "RAISE_ACTION = 1 \n",
    "RAISE_PENALTY = 0.5\n",
    "# In here penalize the reward by making it expensive a bit\n",
    "class RaisePenaltyWrapper(PettingZooEnv):\n",
    "    def __init__(self, env, positive_penalize=0.5, negative_penalize=1.5):\n",
    "        super().__init__(env)\n",
    "        self.positive_penalize = positive_penalize\n",
    "        self.negative_penalize = negative_penalize\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rewards, terminations, truncations, infos = super().step(action_dict)\n",
    "\n",
    "        new_rewards = {}\n",
    "\n",
    "        for agent_id, r in rewards.items():\n",
    "            a = action_dict.get(agent_id, None)\n",
    "\n",
    "            # 1) small penalty per raise\n",
    "            if a == RAISE_ACTION:\n",
    "                r -= 0.1\n",
    "\n",
    "            # 2) asymmetric win/loss scaling at terminal\n",
    "            done = terminations.get(agent_id, False) or truncations.get(agent_id, False)\n",
    "            if done:\n",
    "                if r > 0:\n",
    "                    r = self.positive_penalize * r\n",
    "                elif r < 0:\n",
    "                    r = self.negative_penalize * r\n",
    "\n",
    "            new_rewards[agent_id] = r\n",
    "        return obs, new_rewards, terminations, truncations, infos\n",
    "\n",
    "\n",
    "def env_creator(config=None):\n",
    "    # raw PettingZoo AEC env\n",
    "    env = texas_holdem_v4.env()\n",
    "    env.reset(seed=(config or {}).get(\"seed\", None))\n",
    "    # RLlib wrapper so it looks like a MultiAgentEnv\n",
    "    #env = PettingZooEnv(env)\n",
    "    wrapper_env = RaisePenaltyWrapper(\n",
    "        env,\n",
    "        positive_penalize=0.75,\n",
    "        negative_penalize=1.5,\n",
    "    )\n",
    "    return wrapper_env\n",
    "\n",
    "env_name = \"texas_holdem_v4\"\n",
    "register_env(env_name, lambda config: env_creator(config))\n",
    "\n",
    "# Create one test env to grab spaces\n",
    "test_env = env_creator()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0f63e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs['observation'].shape: (72,)\n",
      "obs['action_mask']: [1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_env = texas_holdem_v4.env(render_mode=\"human\")\n",
    "test_env.reset()\n",
    "obs, reward, termination, truncation, info = test_env.last()\n",
    "\n",
    "print(\"obs['observation'].shape:\", obs[\"observation\"].shape)\n",
    "print(\"obs['action_mask']:\", obs[\"action_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704d191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-25 21:54:38,726 E 6405 4628719] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import texas_holdem_v4\n",
    "\n",
    "env = texas_holdem_v4.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        mask = observation[\"action_mask\"]\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample(mask)\n",
    "\n",
    "    env.step(action)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352d1b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('player_0': Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (72,), float32)), 'player_1': Dict('action_mask': Box(0, 1, (4,), int8), 'observation': Box(0.0, 1.0, (72,), float32)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90a013a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('player_0': Discrete(4), 'player_1': Discrete(4))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "import gymnasium as gym\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "\n",
    "class MaskedDQNTorchRLModule(TorchRLModule):\n",
    "    def __init__(self, observation_space, action_space, model_config, **kwargs):\n",
    "        super().__init__(\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "            model_config=model_config,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # obs_space is a Dict with keys \"observation\" and \"action_mask\"\n",
    "        obs_box = observation_space[\"observation\"]\n",
    "        self.mask_dim = action_space.n\n",
    "\n",
    "        # Use obs_box directly, no manual flatten-with-mask trick\n",
    "        obs_low = obs_box.low\n",
    "        obs_high = obs_box.high\n",
    "\n",
    "        self.base_model = FullyConnectedNetwork(\n",
    "            obs_space=obs_box,\n",
    "            action_space=action_space,\n",
    "            num_outputs=self.mask_dim,\n",
    "            model_config=model_config,\n",
    "            name=\"masked_dqn_fcnet\",\n",
    "        )\n",
    "\n",
    "    def _forward_inference(self, batch, **kwargs):\n",
    "        return self._forward_masked(batch, explore=False)\n",
    "\n",
    "    def _forward_exploration(self, batch, **kwargs):\n",
    "        return self._forward_masked(batch, explore=True)\n",
    "\n",
    "    def _forward_train(self, batch, **kwargs):\n",
    "        # Training uses Q-values only\n",
    "        out = self._forward_masked(batch, explore=False)\n",
    "        return {\"q_values\": out[\"q_values\"]}\n",
    "\n",
    "    # def _forward_single(self, batch):\n",
    "    #     out = {}\n",
    "    #     for agent_id, obs in batch.items():\n",
    "    #         out[agent_id] = self._forward_single(obs)\n",
    "    #     return out\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            t = x\n",
    "        else:\n",
    "            t = torch.as_tensor(x, dtype=torch.float32)\n",
    "\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(0)\n",
    "\n",
    "        return t.to(next(self.parameters()).device)\n",
    "\n",
    "    def _forward_masked(self, batch, explore: bool):\n",
    "        # batch[\"obs\"] is Dict-space: {\"observation\": ..., \"action_mask\": ...}\n",
    "        # out = {}\n",
    "        # for agent_id, obs in batch.items():\n",
    "        #     out[agent_id] = self._forward_single(obs)\n",
    "        obs_dict = batch[\"obs\"]\n",
    "        #print(obs_dict)\n",
    "\n",
    "        obs = obs_dict[\"observation\"]#.float()\n",
    "        raw_mask = obs_dict[\"action_mask\"]\n",
    "        mask = self._to_tensor(raw_mask)\n",
    "\n",
    "        obs = self._to_tensor(obs)    # [B, obs_dim]\n",
    "\n",
    "        # Q-values from base FC network\n",
    "        logits, _ = self.base_model({\"obs\": obs}, [], None)\n",
    "\n",
    "        # Convert mask to 0/1\n",
    "        legal = (mask > 0.5).float()\n",
    "\n",
    "        # Mask illegal actions with -inf\n",
    "        inf = torch.finfo(logits.dtype).min\n",
    "        masked_q = logits + (1.0 - legal) * inf\n",
    "        print(masked_q)\n",
    "\n",
    "        if explore:\n",
    "            # epsilon-greedy sampling\n",
    "            eps = 0.05  # you can tune\n",
    "            rand = torch.rand(masked_q.shape[0], device=masked_q.device)\n",
    "            greedy = torch.argmax(masked_q, dim=-1)\n",
    "\n",
    "            # random legal action\n",
    "            legal_indices = legal.nonzero(as_tuple=False)\n",
    "            random_actions = torch.zeros_like(greedy)\n",
    "            for i in range(mask.shape[0]):\n",
    "                valid = torch.where(mask[i] > 0.5)[0]\n",
    "                random_actions[i] = valid[torch.randint(len(valid), (1,))]\n",
    "\n",
    "            actions = torch.where(rand < eps, random_actions, greedy)\n",
    "\n",
    "        else:\n",
    "            # greedy action\n",
    "            actions = torch.argmax(masked_q, dim=-1)\n",
    "\n",
    "        # RLlib DQN Learner EXPECTS this structure:\n",
    "        return {\n",
    "            \"q_values\": masked_q,\n",
    "            \"action_distribution_inputs\": masked_q,  # DQN uses Q-values as dist inputs\n",
    "            \"actions\": actions,\n",
    "            \"actions_for_env\": actions,  # connectors need this\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c17c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-multi-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 22:44:17,213\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import gymnasium as gym\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "\n",
    "\n",
    "config = DQNConfig()\n",
    "config.api_stack(  # <-- turn ON the new stack explicitly\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "config = (\n",
    "    config\n",
    "    .environment(env=env_name)\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=0, create_env_on_local_worker=True)\n",
    "    .rl_module(\n",
    "        rl_module_spec= RLModuleSpec(\n",
    "        module_class=MaskedDQNTorchRLModule, \n",
    "        model_config={}, \n",
    "        )\n",
    "    )\n",
    "    .training(\n",
    "        replay_buffer_config = {\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 50_000,\n",
    "            \"prioritized_replay_alpha\": 0.6,\n",
    "            \"prioritized_replay_beta\": 0.4,\n",
    "            \"prioritized_replay_eps\": 1e-6,\n",
    "        },\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        train_batch_size=64,\n",
    "        minibatch_size=32,\n",
    "        #entropy_coeff=0.0001,  #penalize the deterministic policy\n",
    "        #entropy_coeff_schedule=None,\n",
    "        # model={\n",
    "        #     #\"custom_model\": FlatActionMaskingModel,\n",
    "        #     \"use_lstm\": True,\n",
    "        #     \"fcnet_hiddens\": [512, 512],\n",
    "        #     \"fcnet_activation\": \"tanh\",\n",
    "        #     \"_disable_preprocessor_api\": True,\n",
    "        # }\n",
    "        \n",
    "    )\n",
    "    .multi_agent(\n",
    "        # policies={\n",
    "        #     \"shared_policy\": PolicySpec(\n",
    "        #         observation_space=obs_space[\"player_0\"],\n",
    "        #         action_space=act_space[\"player_0\"],\n",
    "        #         config={},\n",
    "        #     ),\n",
    "        # },\n",
    "        # policy_mapping_fn=lambda agent_id, *a, **k: \"shared_policy\",\n",
    "\n",
    "        policies={\n",
    "            \"player_0\": PolicySpec(observation_space=obs_space[\"player_0\"],\n",
    "                action_space=act_space['player_0'],\n",
    "                config={},),\n",
    "            \"player_1\": PolicySpec(observation_space=obs_space[\"player_0\"],\n",
    "                action_space=act_space['player_1'],\n",
    "                config={},),\n",
    "        },\n",
    "        # in this env the agent_ids are literally \"player_0\"/\"player_1\"\n",
    "        policy_mapping_fn=lambda agent_id, *a, **k: agent_id,\n",
    "    )\n",
    "    #.experimental(_validate_config=False)\n",
    ")\n",
    "\n",
    "\n",
    "algo = config.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4695c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0.], dtype=float32),\n",
       " 'action_mask': array([1, 1, 1, 0], dtype=int8)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[\"player_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98ff4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7183e-03, -2.4676e-03, -2.7701e-03, -3.4028e+38]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "{'q_values': tensor([[-3.7183e-03, -2.4676e-03, -2.7701e-03, -3.4028e+38]],\n",
      "       grad_fn=<AddBackward0>), 'action_distribution_inputs': tensor([[-3.7183e-03, -2.4676e-03, -2.7701e-03, -3.4028e+38]],\n",
      "       grad_fn=<AddBackward0>), 'actions': tensor([1]), 'actions_for_env': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "player0_input = {\"obs\": obs[\"player_0\"]}\n",
    "\n",
    "output = algo.get_module(\"player_0\").forward_exploration(player0_input, explore = False)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "355e8bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('player_1', {'observation': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0.], dtype=float32), 'action_mask': array([1, 1, 1, 0], dtype=int8)})])\n",
      "tensor([[-1.5070e-03, -1.1657e-03,  3.8397e-03, -3.4028e+38]])\n",
      "2\n",
      "Episode length: 1\n",
      "Episode rewards: {'player_0': np.float64(0.375), 'player_1': np.float64(-0.75)}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "env = env_creator({})\n",
    "\n",
    "def select_action(module, agent_obs, explore=False):\n",
    "    # Build batch of size 1 for RLModule\n",
    "    batch = {\n",
    "        \"obs\": {\n",
    "            \"observation\": torch.tensor(agent_obs[\"observation\"])[None, :].float(),\n",
    "            \"action_mask\": torch.tensor(agent_obs[\"action_mask\"])[None, :].float(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = (\n",
    "            module.forward_exploration(batch)\n",
    "            if explore\n",
    "            else module.forward_inference(batch)\n",
    "        )\n",
    "\n",
    "    # DQN RLModule emits q_values\n",
    "    qvals = out[\"q_values\"]        # shape [1, num_actions]\n",
    "    return int(torch.argmax(qvals, dim=-1)[0].item())\n",
    "\n",
    "\n",
    "def eval_one_episode(render=False):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    modules = {\n",
    "        \"player_0\": algo.get_module(\"player_0\"),\n",
    "        \"player_1\": algo.get_module(\"player_1\"),\n",
    "    }\n",
    "    # Start empty; we'll add agents as we see them\n",
    "    ep_reward = collections.defaultdict(float)\n",
    "    length = 0\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        actions = {}\n",
    "        print(obs.items())\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            action = select_action(modules[agent_id], agent_obs)\n",
    "            print(action)\n",
    "            actions[agent_id] = int(action)  # Discrete(4) -> int\n",
    "\n",
    "        obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "        length += 1\n",
    "\n",
    "        # accumulate rewards; skip special keys like \"__all__\"\n",
    "        for aid, r in rewards.items():\n",
    "            if aid == \"__all__\":\n",
    "                continue\n",
    "            ep_reward[aid] += r\n",
    "\n",
    "        # episode done: use \"__all__\" if provided, else fall back\n",
    "        if \"__all__\" in terminated:\n",
    "            done = terminated[\"__all__\"] or truncated.get(\"__all__\", False)\n",
    "        else:\n",
    "            done = all(terminated.values()) or all(truncated.values())\n",
    "\n",
    "    return dict(ep_reward), length\n",
    "\n",
    "# Run ONE eval episode\n",
    "ep_rew, ep_len = eval_one_episode(render=False)\n",
    "print(\"Episode length:\", ep_len)\n",
    "print(\"Episode rewards:\", ep_rew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79ef0592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = []\n",
    "lengths = []\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "import torch\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_handle = display(None, display_id=True)\n",
    "\n",
    "def plot_metrics(rew_player0, rew_player1, lengths, curr_iter=0, sum_iter=0, shared=False):\n",
    "    fig = plt.figure(1, figsize=(16, 8))\n",
    "    plt.clf()\n",
    "\n",
    "    # Subplots\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # ---- Reward subplot ----\n",
    "    if shared:\n",
    "        ax1.set_title(f\"Mean Rewards {curr_iter}/{sum_iter}\")\n",
    "        ax1.set_xlabel(\"Eval Interval\")\n",
    "        ax1.set_ylabel(\"Mean Reward\")\n",
    "\n",
    "        ax1.plot(rew_player0, label=\"player_0\", color=\"blue\")\n",
    "        # ax1.plot(rew_player1, label=\"player_1\", color=\"red\")\n",
    "        ax1.legend()\n",
    "    else:\n",
    "        ax1.set_title(f\"Mean Rewards (2 Agents) {curr_iter}/{sum_iter}\")\n",
    "        ax1.set_xlabel(\"Eval Interval\")\n",
    "        ax1.set_ylabel(\"Mean Reward\")\n",
    "\n",
    "        ax1.plot(rew_player0, label=\"player_0\", color=\"blue\")\n",
    "        ax1.plot(rew_player1, label=\"player_1\", color=\"red\")\n",
    "        ax1.legend()\n",
    "\n",
    "    # ---- Episode length subplot ----\n",
    "    ax2.set_title(f\"Mean Episode Length {curr_iter}/{sum_iter}\")\n",
    "    ax2.set_xlabel(\"Eval Interval\")\n",
    "    ax2.set_ylabel(\"Episode Length\")\n",
    "    ax2.plot(lengths, label=\"episode length\", color=\"green\")\n",
    "\n",
    "    # Display\n",
    "    display(fig)\n",
    "    #plot_handle.update(fig)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6038d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_reward = []\n",
    "a2_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5ad3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1500 [00:00<?, ?iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.7337e-04, -3.9633e-03, -1.7375e-03, -3.4028e+38]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_multi_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mTRAINING_ITERATIONS, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRAINING_ITERATIONS):\n\u001b[0;32m----> 8\u001b[0m         \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m EVAL_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m# metrics = algo.evaluate()['evaluation']\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;66;03m# rewards.append(metrics['episode_reward_mean'])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;66;03m# lengths.append(metrics['episode_len_mean'])\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             metrics \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:331\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:1240\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m-> 1240\u001b[0m         train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m         (\n\u001b[1;32m   1243\u001b[0m             train_results,\n\u001b[1;32m   1244\u001b[0m             train_iter_ctx,\n\u001b[1;32m   1245\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_old_api_stack()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:3629\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, TRAINING_STEP_TIMER)):\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TimerAndPrometheusLogger(\n\u001b[1;32m   3627\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_training_step_time\n\u001b[1;32m   3628\u001b[0m     ):\n\u001b[0;32m-> 3629\u001b[0m         training_step_return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m     has_run_once \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;66;03m# On the new API stack, results should NOT be returned anymore as\u001b[39;00m\n\u001b[1;32m   3633\u001b[0m \u001b[38;5;66;03m# a dict, but purely logged through the `MetricsLogger` API. This\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# way, we make sure to never miss a single stats/counter/timer\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# when calling `self.training_step()` more than once within the same\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;66;03m# iteration.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py:646\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_old_api_stack()\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# New API stack (RLModule, Learner, EnvRunner, ConnectorV2).\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_new_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py:668\u001b[0m, in \u001b[0;36mDQN._training_step_new_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# Add the sampled experiences to the replay buffer.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, REPLAY_BUFFER_ADD_DATA_TIMER)):\n\u001b[0;32m--> 668\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_replay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    671\u001b[0m     current_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpeek(\n\u001b[1;32m    673\u001b[0m             (ENV_RUNNER_RESULTS, NUM_AGENT_STEPS_SAMPLED_LIFETIME), default\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m    674\u001b[0m         )\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    675\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ray_new/lib/python3.10/site-packages/ray/rllib/utils/replay_buffers/multi_agent_replay_buffer.py:224\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer.add\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Handle everything as if multi-agent.\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_multi_agent\u001b[49m()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch_timer:\n\u001b[1;32m    227\u001b[0m     pids_and_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_split_into_policy_batches(batch)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_multi_agent'"
     ]
    }
   ],
   "source": [
    "TRAINING_ITERATIONS = 1500\n",
    "import pickle\n",
    "EVAL_INTERVAL = 50\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total=TRAINING_ITERATIONS, desc=\"Training\", unit=\"iter\") as pbar:\n",
    "    for i in range(TRAINING_ITERATIONS):\n",
    "        algo.train()\n",
    "\n",
    "        if (i+1) % EVAL_INTERVAL == 0:\n",
    "            # metrics = algo.evaluate()['evaluation']\n",
    "            # rewards.append(metrics['episode_reward_mean'])\n",
    "            # lengths.append(metrics['episode_len_mean'])\n",
    "            metrics = algo.evaluate()\n",
    "            metrics = metrics[\"env_runners\"]\n",
    "\n",
    "            # r_mean = metrics[\"episode_return_mean\"]\n",
    "            a1_reward.append(metrics[\"policy_reward_mean\"][\"player_0\"])\n",
    "            a2_reward.append(metrics[\"policy_reward_mean\"][\"player_1\"])\n",
    "            l_mean = metrics[\"episode_len_mean\"]\n",
    "\n",
    "            # rewards.append(r_mean)\n",
    "            lengths.append(l_mean)\n",
    "            plot_metrics(a1_reward, a2_reward, lengths, i, TRAINING_ITERATIONS)\n",
    "            algo.save(\"./checkpoints_3_DQN\")\n",
    "        #pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd8b3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SUITS = [\"Spades\", \"Hearts\", \"Diamonds\", \"Clubs\"]\n",
    "RANKS = [\"A\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\"]\n",
    "\n",
    "def decode_visible_cards(obs_dict):\n",
    "    \"\"\"Return list of human-readable cards visible to this player.\"\"\"\n",
    "    vec = obs_dict[\"observation\"]\n",
    "    # ensure it's a flat array\n",
    "    vec = np.array(vec).astype(int)\n",
    "\n",
    "    cards_bits = vec[:52]\n",
    "    cards = []\n",
    "    for i, bit in enumerate(cards_bits):\n",
    "        if bit == 1:\n",
    "            suit = SUITS[i // 13]\n",
    "            rank = RANKS[i % 13]\n",
    "            cards.append(f\"{rank} of {suit}\")\n",
    "    return cards\n",
    "ACTION_NAMES = {\n",
    "    0: \"Call\",\n",
    "    1: \"Raise\",\n",
    "    2: \"Fold\",\n",
    "    3: \"Check\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0106626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Call\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Call\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['2 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['2 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Call\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['6 of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '6 of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Raise\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['6 of Hearts', '2 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Call\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '6 of Hearts', '2 of Diamonds', '6 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Check\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['6 of Hearts', '2 of Diamonds', '6 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: 0\n",
      "Chosen action: Check\n",
      "-------------------\n",
      "-------------------\n",
      "player_0\n",
      "Visible cards: ['6 of Hearts', '2 of Diamonds', '6 of Diamonds', '10 of Diamonds', 'A of Clubs', '8 of Clubs', '9 of Clubs']\n",
      "Reward so far: -13.0\n",
      "-------------------\n",
      "-------------------\n",
      "player_1\n",
      "Visible cards: ['A of Hearts', '6 of Hearts', '2 of Diamonds', '6 of Diamonds', '10 of Diamonds', 'A of Clubs', '5 of Clubs']\n",
      "Reward so far: 13.0\n",
      "-------------------\n",
      "Saved to ppo_poke.gif\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "rmode = \"rgb_array\"\n",
    "\n",
    "env = texas_holdem_v4.env(render_mode=rmode)\n",
    "env.reset()\n",
    "frames = []\n",
    "for agent in env.agent_iter():\n",
    "    print(\"-------------------\")\n",
    "    print(agent)\n",
    "    obs, reward, termination, truncation, info = env.last()\n",
    "    # print(observation)\n",
    "    visible_cards = decode_visible_cards(obs)\n",
    "    print(\"Visible cards:\", visible_cards)\n",
    "    print(\"Reward so far:\", reward)\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "        print(\"-------------------\")\n",
    "    else:\n",
    "        rllib_obs = {\n",
    "                \"action_mask\": obs[\"action_mask\"],\n",
    "                \"observation\": obs[\"observation\"],\n",
    "            }\n",
    "\n",
    "        # policy_id MUST match the agent name (\"player_0\" / \"player_1\")\n",
    "        action = algo.compute_single_action(\n",
    "                rllib_obs,\n",
    "                policy_id= agent,#\"shared_policy\",\n",
    "                explore=False,      # deterministic eval\n",
    "        )\n",
    "        action_name = ACTION_NAMES.get(int(action), f\"Unknown({action})\")\n",
    "\n",
    "        print(\"Chosen action:\", action_name)\n",
    "        print(\"-------------------\")\n",
    "    if rmode == \"rgb_array\":\n",
    "        frame = env.render()\n",
    "        if frame is not None:\n",
    "            frames.append(frame)\n",
    "    env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "if rmode != \"human\":\n",
    "   imageio.mimsave(\"ppo_poke.gif\", frames, fps=2)\n",
    "   print(\"Saved to ppo_poke.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
